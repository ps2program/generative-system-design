{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from flask import Flask, request, Response\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Simulated StateMemory class\n",
    "class StateMemory:\n",
    "    def __init__(self):\n",
    "        self.requirements = {}\n",
    "        self.physicals = {}\n",
    "\n",
    "    def get_state_as_json(self):\n",
    "        return {\n",
    "            \"requirements\": {req_id: req.__dict__ for req_id, req in self.requirements.items()},\n",
    "            \"physicals\": {phy_id: phy.__dict__ for phy_id, phy in self.physicals.items()}\n",
    "        }\n",
    "\n",
    "\n",
    "# Requirement and Physical classes\n",
    "class Requirement:\n",
    "    def __init__(self, name, description):\n",
    "        self.id = str(time.time())  # Simulated unique ID\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "\n",
    "\n",
    "class Physical:\n",
    "    def __init__(self, name, description, parent_id):\n",
    "        self.id = str(time.time())  # Simulated unique ID\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.parent_id = parent_id\n",
    "\n",
    "\n",
    "# LLMProcessor class\n",
    "class LLMProcessorParallel:\n",
    "    def __init__(self, llm_client, state_memory):\n",
    "        self.llm_client = llm_client\n",
    "        self.state_memory = state_memory\n",
    "\n",
    "    def process_requirement(self, requirement):\n",
    "        physical_data = self.call_llm_to_create_physical(requirement)\n",
    "\n",
    "        physical = Physical(\n",
    "            name=physical_data[\"name\"],\n",
    "            description=physical_data[\"description\"],\n",
    "            parent_id=requirement.id\n",
    "        )\n",
    "        self.state_memory.physicals[physical.id] = physical\n",
    "\n",
    "        created_physical = self.state_memory.physicals[physical.id]\n",
    "\n",
    "        return {\n",
    "            \"requirement\": requirement.name,\n",
    "            \"physical_name\": created_physical.name,\n",
    "            \"description\": created_physical.description\n",
    "        }\n",
    "\n",
    "    def call_llm_to_create_physical(self, requirement):\n",
    "        prompt = f\"Create a physical design for the following requirement:\\n\\n{requirement.description}\\n. each should be not more than 10 words\"\n",
    "        response = self.llm_client(prompt)\n",
    "        return {\n",
    "            \"name\": f\"Physical for {requirement.name}\",\n",
    "            \"description\": response\n",
    "        }\n",
    "\n",
    "\n",
    "@app.route('/process-requirements', methods=['POST'])\n",
    "def process_requirements():\n",
    "    data = request.get_json()\n",
    "\n",
    "    if not data or \"requirements\" not in data:\n",
    "        return jsonify({\"error\": \"Invalid input, 'requirements' key is required\"}), 400\n",
    "\n",
    "    requirements = []\n",
    "    for req in data[\"requirements\"]:\n",
    "        if \"name\" in req and \"description\" in req:\n",
    "            requirements.append(Requirement(req[\"name\"], req[\"description\"]))\n",
    "        else:\n",
    "            return jsonify({\"error\": \"Each requirement must have 'name' and 'description'\"}), 400\n",
    "\n",
    "    llm_client = create_mistral_model(\"Mistral-Model\")\n",
    "    state_memory = StateMemory()\n",
    "    llm_processor = LLMProcessorParallel(llm_client, state_memory)\n",
    "\n",
    "    def generate():\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(llm_processor.process_requirement, requirement)\n",
    "                for requirement in requirements\n",
    "            ]\n",
    "\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    yield f\"data: {result}\\n\\n\"  # Server-Sent Event format\n",
    "                except Exception as e:\n",
    "                    yield f\"data: {{'error': 'An error occurred: {str(e)}'}}\\n\\n\"\n",
    "\n",
    "    return Response(generate(), content_type='text/event-stream')\n",
    "\n",
    "\n",
    "def create_mistral_model(model_name):\n",
    "    return ChatOpenAI(\n",
    "        api_key=os.getenv(\"NETVIBES_API_KEY\"),\n",
    "        base_url=os.getenv(\"NETVIBES_BASE_URL\"),\n",
    "        model=os.getenv(\"NETVIBES_OPENAI_MISTRAL_DEPLOYMENT_NAME\"),\n",
    "        max_tokens=5000,\n",
    "        name=model_name,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
